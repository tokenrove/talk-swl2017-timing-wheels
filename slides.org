#+REVEAL_ROOT: http://cdn.jsdelivr.net/reveal.js/3.0.0/
#+REVEAL_THEME: white
#+OPTIONS: toc:nil
#+OPTIONS: num:nil
#+TITLE: Implementations of Timing Wheels
#+AUTHOR: Julian Squires
#+EMAIL:

* Implementations of Timing Wheels

Julian Squires <julian.squires@adgear.com>

#+BEGIN_NOTES
(1 minute -- 120 words?)

Hi.  Today I'm going to talk about a class of Systems I Love, timer
facilities.  As we'll see, efficiently providing timers is a classic
systems problem that looks simple on the outside, but is filled with
pragmatic tradeoffs and opportunities to win in special cases.

For lack of time, I'm going to talk about performance without any
benchmarks or evidence, which is as pure a form of lying as any, so I
hope you'll forgive me and take everything with a grain of salt.

Where I say "x performs better than y", you should hear "x probably
performs better than y, let's be sure to measure it".

Along the way we encounter many favorite pragmatic data structures:
ring buffers, histograms, hash tables, bitmaps, and heaps.

The key idea to take away here is that efficient system design always
involves tradeoffs, which means there will always be room for new
wheels.  In this case, we'll study a specific case of reinvented
wheels: timing wheels.

When we speak of reinventing wheels, we're rarely actually talking
about wheels.

The other disclaimer here is that I have only made cursory glances at
many of the codebases from which I'm drawing examples, so I hope I
don't offend anyone by mischaracterizing an implementation I was too
dumb to grasp.
#+END_NOTES

*

Varghese and Lauck. "Hashed and hierarchical timing
wheels: Data structures for the efficient implementation of a timer
facility." ACM SIGOPS Operating Systems Review 21.5 (1987): 25-38.

Varghese and Lauck. "Hashed and hierarchical timing
wheels: efficient data structures for implementing a timer facility."
IEEE/ACM transactions on networking 5.6 (1997): 824-834.

Varghese. Network Algorithmics: An Interdisciplinary Approach
To Designing Fast Networked Devices. Morgan Kaufmann, 2004.

#+BEGIN_NOTES
They would have used these techniques on DEC's Gigaswitch.

In 1997, they published an updated version of the paper, with notes on
the implementation in BSD, a comparison with calendar queues, and the
first hints of SMP considerations.

In 2004, Varghese published the excellent Network Algorithmics book
which covers the content of the paper in reasonable depth.
#+END_NOTES

* Some recent activity on timing wheels

https://lwn.net/Articles/646950/

https://blog.acolyer.org/2015/11/23/hashed-and-hierarchical-timing-wheels/

https://www.snellman.net/blog/archive/2016-07-27-ratas-hierarchical-timer-wheel/


#+BEGIN_NOTES
Three great articles all of which about during the time I was thinking
about efficient structures for timing.  The first is an LWN article
about proposed changes to the Linux timing wheel implementation,
merged in 4.8.

The second, Adrian Colyer's excellent Morning Paper blog, covering the
original paper.

And the third, about a year after the first one, Juho Snellman's
discussion of his timing wheel implementation, ratas.
#+END_NOTES


* Motivation

| Optimistic      | Pessimistic            |
|-----------------+------------------------|
| flow control    | network timeouts       |
| circuit breaker | TCP congestion control |
| simulations     | IO timeouts            |

#+BEGIN_NOTES
(2 minutes - 250 words?)

- having cheaply-cancellable timers changes what you can do
- lots of distributed systems algorithms

- basic example we'll run with: best-effort UDP application that needs
  timeouts; these timers are almost always cancelled

- flow-control produces timers that almost always expire

- failure recovery, rate-based flow control, scheduling, ...

In a videogame or other simulation, one might want to schedule the
behavior of many actors based on their next think time, with the
ability to cancel their next scheduling in reaction to events.

One question that came up when I talked about this before was what
about cron?  Although I certainly hope your cron daemon isn't handling
millions of scheduled events, it turns out the traditional approach
there is an event list due to Franta and Maly; this evolved into
calendar queues which we'll talk about later.
#+END_NOTES

* Interface

#+BEGIN_EXAMPLE
user:
  start-timer(interval, request-id, expiry-action)
  stop-timer(request-id)
interrupt:
  per-tick-bookkeeping
internal:
  expiry-processing
#+END_EXAMPLE

#+BEGIN_NOTES
Varghese proposes the following interface for a timer system, where we
can start and stop timers, supplying a request ID to identify them,
and where the timer interrupt calls per-tick-bookkeeping, which ends
up calling expiry-processing if necessary.
#+END_NOTES

* Interface

#+BEGIN_EXAMPLE
user:
  schedule(duration, action) -> timer
  schedule(range, action) -> timer
  timer.cancel()
interrupt:
  advance(duration, max_work)
internal:
  till_next_wake() -> duration
#+END_EXAMPLE

#+BEGIN_NOTES
This doesn't quite map to the implementations we'll study here, and
I'll use something closer to Juho Snellman's Ratas, which uses
schedule, cancel, and advance.  We'll assume that you have a
programmable timer interrupt of some sort that can call advance when
you need it.
#+END_NOTES

* Why study timers?

Understand:
- accuracy of measurement
- billing / quotas
- security scenarios

#+BEGIN_NOTES
We'll see there are tradeoffs here that affect what you, a user of
such a facility, can do.  You should know how your system implements
timers (possibly several kinds), and what might introduce performance
problems, or confound measurement in benchmarks.  If you use timing in
a way that relates to money (billing by time spent, for example), can
an adversary use deficiencies in your timing facility's implementation
to affect your bottom line?

What if we wanted to make an expiry-oriented database?
#+END_NOTES

* Guarantees of a timer system

A timer scheduled to fire after *t* ticks will have its action
executed, some time after *t* ticks (if your clock is monotonic and
doesn't jump forward or skew forward and ...)

#+BEGIN_NOTES
The truth is, as soon as you've involved time in a system, you've
already lost, to some extent.  But the fact that time is your enemy is
a subject for a dozen Systems We Love talks, so we'll take this stuff
for granted here.

Timer slack and other coalescing techniques help us avoid wakeups; on
the other hand, they cause stampedes and inaccurate expiry.

Just look at the problems involved in getting accurate measurements
from perf, instruction skid, and so on.
#+END_NOTES

* Considerations for a timer subsystem

 - optimistic versus pessimistic
 - accuracy versus performance
 - ranged scheduling versus exact
 - repeating versus one-shot
 - throughput versus latency
   - bounded work per timer interrupt
   - timer stampede

#+BEGIN_NOTES
 - how sloppy can timer execution be?
 - would the user be happy with providing a range?
 - are some or all timers periodic?

 - what about timer stampede?
   - see optimal probabilistic cache stampede prevention: http://www.vldb.org/pvldb/vol8/p886-vattani.pdf
   - ratas allows you to specify a maximum number of timers to expire at once
   - you could also do this when adding the timer instead of when
     expiring, and this would allow you to have a fixed allocation for
     timers
#+END_NOTES

* How can we implement a timer facility?

* Unordered Lists

illustration here

* Ordered Lists

illustration here

* Zephyr

http://zephyrproject.org/

* Zephyr: add timer

~kernel/include/timeout_q.h~

#+BEGIN_SRC c
/*
 * Add timeout to timeout queue. Record waiting thread and wait queue if any.
 *
 * Cannot handle timeout == 0 and timeout == K_FOREVER.
 *
 * Must be called with interrupts locked.
 */

static inline void _add_timeout(struct k_thread *thread,
                                struct _timeout *timeout,
                                _wait_q_t *wait_q,
                                int32_t timeout_in_ticks)
{
        __ASSERT(timeout_in_ticks > 0, "");

        timeout->delta_ticks_from_prev = timeout_in_ticks;
        timeout->thread = thread;
        timeout->wait_q = (sys_dlist_t *)wait_q;

        K_DEBUG("before adding timeout %p\n", timeout);
        _dump_timeout(timeout, 0);
        _dump_timeout_q();

        int32_t *delta = &timeout->delta_ticks_from_prev;
        sys_dnode_t *node;

        SYS_DLIST_FOR_EACH_NODE(&_timeout_q, node) {
                struct _timeout *in_q = (struct _timeout *)node;

                if (*delta <= in_q->delta_ticks_from_prev) {
                        in_q->delta_ticks_from_prev -= *delta;
                        sys_dlist_insert_before(&_timeout_q, node,
                                                &timeout->node);
                        goto inserted;
                }

                *delta -= in_q->delta_ticks_from_prev;
        }

        sys_dlist_append(&_timeout_q, &timeout->node);

inserted:
        K_DEBUG("after adding timeout %p\n", timeout);
        _dump_timeout(timeout, 0);
        _dump_timeout_q();
}
#+END_SRC

* Zephyr: next timer

~kernel/timer.c~

#+BEGIN_SRC c
int32_t _timeout_remaining_get(struct _timeout *timeout)
{
        unsigned int key = irq_lock();
        int32_t remaining_ticks;

        if (timeout->delta_ticks_from_prev == _INACTIVE) {
                remaining_ticks = 0;
        } else {
                /*
                 * compute remaining ticks by walking the timeout list
                 * and summing up the various tick deltas involved
                 */
                struct _timeout *t =
                        (struct _timeout *)sys_dlist_peek_head(&_timeout_q);

                remaining_ticks = t->delta_ticks_from_prev;
                while (t != timeout) {
                        t = (struct _timeout *)sys_dlist_peek_next(&_timeout_q,
                                                                   &t->node);
                        remaining_ticks += t->delta_ticks_from_prev;
                }
        }

        irq_unlock(key);
        return __ticks_to_ms(remaining_ticks);
}
#+END_SRC

#+BEGIN_NOTES
dlist here is a simple doubly-linked list.
#+END_NOTES

* Zephyr: cancel

~kernel/include/timeout_q.h~

#+BEGIN_SRC c
/* returns _INACTIVE if the timer is not active */
static inline int _abort_timeout(struct _timeout *timeout)
{
        if (timeout->delta_ticks_from_prev == _INACTIVE) {
                return _INACTIVE;
        }

        if (!sys_dlist_is_tail(&_timeout_q, &timeout->node)) {
                sys_dnode_t *next_node =
                        sys_dlist_peek_next(&_timeout_q, &timeout->node);
                struct _timeout *next = (struct _timeout *)next_node;

                next->delta_ticks_from_prev += timeout->delta_ticks_from_prev;
        }
        sys_dlist_remove(&timeout->node);
        timeout->delta_ticks_from_prev = _INACTIVE;

        return 0;
}
#+END_SRC

* Darwin

http://opensource.apple.com/

* Darwin: scheduling a timer

~osfmk/kern/call_entry.h~

#+BEGIN_SRC c
static __inline__ queue_head_t *
call_entry_enqueue_deadline(
	call_entry_t			entry,
	queue_head_t			*queue,
	uint64_t			deadline)
{
	queue_t		old_queue = entry->queue;
	call_entry_t	current;

	if (old_queue != queue || entry->deadline < deadline) {
		if (old_queue == NULL) {
			current = CE(queue_first(queue));
		} else if (old_queue != queue) {
			(void)remque(qe(entry));
			current = CE(queue_first(queue));
		} else {
			current = CE(queue_next(qe(entry)));
			(void)remque(qe(entry));
		}

		while (TRUE) {
			if (queue_end(queue, qe(current)) ||
			    deadline < current->deadline) {
				current = CE(queue_prev(qe(current)));
				break;
			}

			current = CE(queue_next(qe(current)));
		}
		insque(qe(entry), qe(current));
	}
	else if (deadline < entry->deadline) {
		current = CE(queue_prev(qe(entry)));

		(void)remque(qe(entry));

		while (TRUE) {
			if (queue_end(queue, qe(current)) ||
			    current->deadline <= deadline) {
				break;
			}

			current = CE(queue_prev(qe(current)));
		}
		insque(qe(entry), qe(current));
	}
	entry->queue = queue;
	entry->deadline = deadline;

	return (old_queue);
}
#+END_SRC

* Darwin

~osfmk/kern/thread_call.c~

#+BEGIN_SRC c
	if (cancel_all)
		result = _remove_from_pending_queue(func, param, cancel_all) |
			_remove_from_delayed_queue(func, param, cancel_all);
	else
		result = _remove_from_pending_queue(func, param, cancel_all) ||
			_remove_from_delayed_queue(func, param, cancel_all);
#+END_SRC

#+BEGIN_NOTES
I would be remiss in not mentioning this piece of the timer code in
macOS.  A nice mix of too-clever and yet highly repetitive.
#+END_NOTES

* Binary heaps

 - libev
 - node.js
 - Illumos

#+BEGIN_NOTES
Whenever we want online sorting, one of the ideas we're going to
immediately think of is a priority queue, and the most obvious way to
implement it is with a heap.
#+END_NOTES

* O(lg(N)) versus O(1)

[[http://t-t-travails.blogspot.ca/2008/07/overzealous-use-of-my-red-black-tree.html][Jason Evans says:]]

#+BEGIN_QUOTE
In essence, my initial failure was to disregard the difference between
a O(1) algorithm and a O(lg n) algorithm. Intuitively, I think of
logarithmic-time algorithms as fast, but constant factors and large n
can conspire to make logarthmic time not nearly good enough.
#+END_QUOTE

#+BEGIN_NOTES
Hashed vs Hierarchical
Big-O notation vs reality
How big is the difference between O(lg(N)) and O(1)?

When we think about the binary logarithm of N, it's pretty small, and
the constant factors are more likely to make a difference than these
asymptotic factors.

I think he's right, but of course the flip side of this is that a
well-implemented logarithmic-time algorithm might be competitive with
a constant-time algorithm.  I became more convinced of this after
looking at the Solaris cyclic subsystem implementation, which is
heap-based, but features some pretty careful cache and per-CPU tuning.

When I started looking at this, I was convinced that a
straight-forward heap solution would not be adequate, and that the
systems which used a heap did so simply "by default", without much
thought.

However, after seeing the careful constant-factor tuning of Solaris's
cyclic subsystem, I am willing to be persuaded that a heap could
perform adequately for a large number of timers.

Accuracy vs Performance
#+END_NOTES

* Heaps: libev

* Heaps: node.js
* Heaps: Illumos

~usr/src/uts/common/os/cyclic.c~

#+BEGIN_NOTES
The description of this system could be a talk or two on its own.

There's a beautiful, huge comment here (as well as some related,
useful ones in the header files), which goes into depth on the care
that was taken in designing this heap to maximize locality, minimize
cache misses, and so on.  Not to mention all the interesting per-CPU
considerations I'm not even getting into here.

One thing about this system compared to some of the others we're
discussing is that it seems to have been designed to support
high-accuracy timers, like Linux's hrtimers which we'll discuss later.

Since this claims to handle this problem in realtime,

The clock, watchdog, and callouts facilities all build on top of
cyclic.
#+END_NOTES

* Illumos: callouts

~usr/src/uts/common/os/callout.c~

#+BEGIN_SRC c
/*
 * Add a new callout list into a callout table's queue in sorted order by
 * expiration.
 */
static int
callout_queue_add(callout_table_t *ct, callout_list_t *cl)
{
        callout_list_t *nextcl;
        hrtime_t expiration;

        expiration = cl->cl_expiration;
        nextcl = ct->ct_queue.ch_head;
        if ((nextcl == NULL) || (expiration < nextcl->cl_expiration)) {
                CALLOUT_LIST_INSERT(ct->ct_queue, cl);
                return (1);
        }

        while (nextcl != NULL) {
                if (expiration < nextcl->cl_expiration) {
                        CALLOUT_LIST_BEFORE(cl, nextcl);
                        return (0);
                }
                nextcl = nextcl->cl_next;
        }
        CALLOUT_LIST_APPEND(ct->ct_queue, cl);

        return (0);
}
#+END_SRC

#+BEGIN_NOTES
Callouts, built on top of this, use an ordered list for their own
queues.
#+END_NOTES


* DPDK

http://dpdk.org/

* DPDK: Skiplists

#+BEGIN_SRC c
static uint32_t
timer_get_skiplist_level(unsigned curr_depth)
{
#ifdef RTE_LIBRTE_TIMER_DEBUG
	static uint32_t i, count = 0;
	static uint32_t levels[MAX_SKIPLIST_DEPTH] = {0};
#endif

	/* probability value is 1/4, i.e. all at level 0, 1 in 4 is at level 1,
	 * 1 in 16 at level 2, 1 in 64 at level 3, etc. Calculated using lowest
	 * bit position of a (pseudo)random number.
	 */
	uint32_t rand = rte_rand() & (UINT32_MAX - 1);
	uint32_t level = rand == 0 ? MAX_SKIPLIST_DEPTH : (rte_bsf32(rand)-1) / 2;

	/* limit the levels used to one above our current level, so we don't,
	 * for instance, have a level 0 and a level 7 without anything between
	 */
	if (level > curr_depth)
		level = curr_depth;
	if (level >= MAX_SKIPLIST_DEPTH)
		level = MAX_SKIPLIST_DEPTH-1;
#ifdef RTE_LIBRTE_TIMER_DEBUG
	count ++;
	levels[level]++;
	if (count % 10000 == 0)
		for (i = 0; i < MAX_SKIPLIST_DEPTH; i++)
			printf("Level %u: %u\n", (unsigned)i, (unsigned)levels[i]);
#endif
	return level;
}
#+END_SRC


#+BEGIN_NOTES
Unfortunate use of randomness which can be inconvenient, although I
understand there are ways to build good skiplists that don't depend on
an adequate PRNG.
#+END_NOTES

* DPDK: schedule timer

#+BEGIN_SRC c
/*
 * add in list, lock if needed
 * timer must be in config state
 * timer must not be in a list
 */
static void
timer_add(struct rte_timer *tim, unsigned tim_lcore, int local_is_locked)
{
	unsigned lcore_id = rte_lcore_id();
	unsigned lvl;
	struct rte_timer *prev[MAX_SKIPLIST_DEPTH+1];

	/* if timer needs to be scheduled on another core, we need to
	 * lock the list; if it is on local core, we need to lock if
	 * we are not called from rte_timer_manage() */
	if (tim_lcore != lcore_id || !local_is_locked)
		rte_spinlock_lock(&priv_timer[tim_lcore].list_lock);

	/* find where exactly this element goes in the list of elements
	 * for each depth. */
	timer_get_prev_entries(tim->expire, tim_lcore, prev);

	/* now assign it a new level and add at that level */
	const unsigned tim_level = timer_get_skiplist_level(
			priv_timer[tim_lcore].curr_skiplist_depth);
	if (tim_level == priv_timer[tim_lcore].curr_skiplist_depth)
		priv_timer[tim_lcore].curr_skiplist_depth++;

	lvl = tim_level;
	while (lvl > 0) {
		tim->sl_next[lvl] = prev[lvl]->sl_next[lvl];
		prev[lvl]->sl_next[lvl] = tim;
		lvl--;
	}
	tim->sl_next[0] = prev[0]->sl_next[0];
	prev[0]->sl_next[0] = tim;

	/* save the lowest list entry into the expire field of the dummy hdr
	 * NOTE: this is not atomic on 32-bit*/
	priv_timer[tim_lcore].pending_head.expire = priv_timer[tim_lcore].\
			pending_head.sl_next[0]->expire;

	if (tim_lcore != lcore_id || !local_is_locked)
		rte_spinlock_unlock(&priv_timer[tim_lcore].list_lock);
}
#+END_SRC

* DPDK: cancel

#+BEGIN_SRC c
/*
 * del from list, lock if needed
 * timer must be in config state
 * timer must be in a list
 */
static void
timer_del(struct rte_timer *tim, union rte_timer_status prev_status,
		int local_is_locked)
{
	unsigned lcore_id = rte_lcore_id();
	unsigned prev_owner = prev_status.owner;
	int i;
	struct rte_timer *prev[MAX_SKIPLIST_DEPTH+1];

	/* if timer needs is pending another core, we need to lock the
	 * list; if it is on local core, we need to lock if we are not
	 * called from rte_timer_manage() */
	if (prev_owner != lcore_id || !local_is_locked)
		rte_spinlock_lock(&priv_timer[prev_owner].list_lock);

	/* save the lowest list entry into the expire field of the dummy hdr.
	 * NOTE: this is not atomic on 32-bit */
	if (tim == priv_timer[prev_owner].pending_head.sl_next[0])
		priv_timer[prev_owner].pending_head.expire =
				((tim->sl_next[0] == NULL) ? 0 : tim->sl_next[0]->expire);

	/* adjust pointers from previous entries to point past this */
	timer_get_prev_entries_for_node(tim, prev_owner, prev);
	for (i = priv_timer[prev_owner].curr_skiplist_depth - 1; i >= 0; i--) {
		if (prev[i]->sl_next[i] == tim)
			prev[i]->sl_next[i] = tim->sl_next[i];
	}

	/* in case we deleted last entry at a level, adjust down max level */
	for (i = priv_timer[prev_owner].curr_skiplist_depth - 1; i >= 0; i--)
		if (priv_timer[prev_owner].pending_head.sl_next[i] == NULL)
			priv_timer[prev_owner].curr_skiplist_depth --;
		else
			break;

	if (prev_owner != lcore_id || !local_is_locked)
		rte_spinlock_unlock(&priv_timer[prev_owner].list_lock);
}
#+END_SRC


* Infinite "Timing Plane"

illustration here of timing plane

#+BEGIN_NOTES
Varghese says (in _Network Algorithmics_):
  Use special techniques for finite universes such as integers

A key insight here is that we don't need the timers to be sorted,
except as they expire.  So that gives us a degree of freedom that
could beat a heap that tries to maintain sorted order on every
operation.

Relationship with radix sort, histogram
#+END_NOTES

* Ring Buffer

The fundamental abstraction of memory: after all, memory itself is a
ring buffer.

* Hashed Timing Wheels

#+BEGIN_NOTES
#+END_NOTES

* *BSD

https://github.com/freebsd/freebsd/blob/master/sys/kern/kern_timeout.c

 - still based on Varghese's code, if comments are to be trusted

https://github.com/DragonFlyBSD/DragonFlyBSD/blob/master/sys/kern/kern_timeout.c
 - Dragonfly has the same thing, but with different per-CPU code

#+BEGIN_NOTES
#+END_NOTES

* Netty, Agrona

https://github.com/netty/netty/blob/4.1/common/src/main/java/io/netty/util/HashedWheelTimer.java
https://github.com/real-logic/Agrona/blob/master/agrona/src/main/java/org/agrona/TimerWheel.java

* Hierarchical Timing Wheels

illustration here

#+BEGIN_NOTES
Indeed, if we want more dynamic range -- log-linear buckets / HDR
histogram, this takes us to hierarchical timing wheels.

In Linux's timing wheel, we also get an appearance by another favorite
practical data structures: bitmaps and compressed bitmaps.

You might notice that the wheels here are split by 6 bits instead
of 8.  Why?  2^6 is 64, so we can represent the occupancy of a wheel
with a 64-bit bitmap.
#+END_NOTES

* Erlang



* Linux

- hrtimers (accurate, run-to-completion) and timeouts (sloppy, usually cancelled)

prctl for thread-wide slack

new one went in in 4.8
https://lwn.net/Articles/646950/
https://lwn.net/Articles/152436/

http://elinux.org/Kernel_Timer_Systems


~linux/kernel/time/timer.c~


* Kafka

http://www.confluent.io/blog/apache-kafka-purgatory-hierarchical-timing-wheels

- formerly used ~java.util.concurrent.DelayQueue~, which is built on a
  binary heap

* timeout.c

http://25thandclement.com/~william/projects/timeout.c.html

* Ratas

#+BEGIN_NOTES
Serendipitously with a reactivated interest I had in timing wheels,
Adrian Colyer's Morning Paper featured Lauck and Varghese's paper, and
Juho Snellman posted about Ratas, a timing wheel implementation he had
designed.

Juho's blog post about the design of Ratas is a great read, and covers
much of what I've tried to get across in this talk.

Snellman makes the argument that we don't need to care about tricks
like bitmaps for finding the next timer, because this will only occur
when the wheel has low occupancy, which he conjectures will be when
the system is under low load and can deal with the extra work of
scanning for the next tick.

I'm not sure if he's right, but it's a compelling idea, especially
since a linear scan of a small array is pretty much the best case in
terms of cache use and prefetching.
#+END_NOTES

* Other techniques

 - calendar queues
 - skip lists
 - softheaps?

#+BEGIN_NOTES
Related, there's been a lot of activity in discrete event simulation
around priority queues usually called calendar queues.
#+END_NOTES

* Calendar Queues

http://stackoverflow.com/questions/6004978/what-is-a-calendar-queue

#+BEGIN_NOTES
- widely studied in Discrete Event Simulation
- many variants
- cron uses Franta-Maly

http://stackoverflow.com/questions/6004978/what-is-a-calendar-queue
#+END_NOTES

* Other interesting stuff

https://blog.acolyer.org/2015/11/24/gd-wheel/

http://www.vldb.org/pvldb/vol8/p886-vattani.pdf

* Fin

#+BEGIN_NOTES
I recently talked about this topic for an hour and twenty minutes, and
today's limit is but twenty minutes, so there is much I was forced to
omit.  I hope this whets your appetite for this lovely family of data
structures; follow the links, and feel free to talk to me afterwards
for more.

I want to thank my employer, AdGear Technologies, for sending me here,
and all of you for listening.  Thank you.
#+END_NOTES
